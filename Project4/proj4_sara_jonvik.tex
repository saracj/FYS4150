\documentclass[a4paper,11pt]{report}
\usepackage[utf8]{inputenc}
%\usepackage[norsk]{babel}
\usepackage{amsmath,amssymb,textcomp,varioref,parskip, color}
\usepackage{listings, hyperref}
\usepackage{graphicx, float}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
%\usepackage[T1]{fontenc}
\usepackage[justification=centering, font=footnotesize]{caption}
\definecolor{green}{rgb}{0, 0.3, 0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{blue}{rgb}{0, 0, 0.8}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\setlength{\jot}{10pt}
\setlength\parindent{0pt}
\lstset{language=Python, 
	basicstyle=\footnotesize,
% 	backgroundcolor=\color{gray},
% 	numberstyle=\color{blue},
	keywordstyle=\color{blue},
	commentstyle=\color{gray},
	stringstyle=\color{green},
	frame=single}
%opening
%opening
\title{Project 4 - Diffusion \\ FYS4150}
\author{Sara J\o nvik}

\graphicspath{{./Data/}}
\newcommand{\colr}[2]{\textcolor{#1}{#2}}
\newcommand{\pdt}[1]{\frac{\partial #1}{\partial t}}
\newcommand{\dt}[1]{\frac{d #1}{d t}}
\newcommand{\ddt}[1]{\frac{d^2 #1}{dt^2}}
\newcommand{\pddx}[1]{\frac{\partial^2 #1}{\partial x^2}}
% \newcommand{\plot}[3]{
% \begin{figure}
% \centering
% \includegraphics[scale=0.5]{#1}
% \caption{#2}
% \label{#3}
% \end{figure}
% }


\begin{document}
\maketitle

\begin{center}
\section*{Abstract}
In this project we used the explicit, implicit and Crank-Nicolson schemes in order to solve the diffusion equation (equation (\ref{eq:diffusion})) which describes the concentration of neurotransmitters across the synaptic cleft separating the cell membranes of two cells. All three methods produced very similar results with some slight deviations from the analytical solution given in section 2. There did not, with our problem and set of \(\Delta x\) and \(\Delta t\), seem to be much advantage in using one method over the other two, they were more or less equally accurate/inaccurate.
\end{center}


\section*{1. Introduction}

In this project we solved the diffusion equation (equation (\ref{eq:diffusion}) ) in order to describe the diffusion of a particular signal molecule, \emph{neurotransmitters}, that transport signals between neurons in the brain. We did this using three different solution method:The explicit scheme (forward Euler), Implicit scheme, and the Crank-Nicolson scheme. All of these methods are based on Taylor expansions, where the Crank Nicolson scheme is a combination of the explicit and implicit schemes. 

\begin{equation}\label{eq:diffusion}
\frac{\partial u}{\partial t} = D \frac{\partial^2 u }{\partial^2 x}
\end{equation}

\textbf{Code used in this project can be found here:} \url{https://github.com/saracj/FYS4150/tree/master/Project4}

\section*{2. Closed Form Solution}

In order to compare the accuracy of the three methods we need the closed form solution to the 1D diffusion equation. We simplify the diffusion equation by splitting it up into two parts: The steady state part, $u_s(x)$, which does not depend on time, and the excess $v(x, t)$, which is time dependent. In doing so we can rewrite the boundary conditions into dirichlet conditions, simplifying both the analytical solution and the numerical implementation. Defining the concentration $u$ as  $u(x, t) = v(x, t) + u_s(x)$, we can solve for the excess $v(x, t)$. 

Since the steady state solution is independent of time we have \(\pdt{u_s} = 0 \):
\[
\pddx{u_s} = 0 \, \,  \longrightarrow \, \, u_s = ax + b
\]

Given the initial condition $u(x, 0) = u_0, \, x \in [0, d]$ and boundary conditions $u(0, t) = d $, $u(d, 0) = 0$ we can ''force'' the stady state solution to take on the given boundary conditions in order to ensure that the excess gains dirichlet conditions: 
\begin{align*}                                                                                                                                                                                                                                                                              
u_s(0) = b &= 1 \qquad &u_s(d) = ad + b = ad + 1 = 0 \\
b &= 1 \qquad &a = -1/d
\end{align*}

Setting $ d= 1$ we see that the steady state solution becomes $u_s(x) = 1 - x$. 

Inserting the expression for $u_s$ in the expression for $u(x, t)$ we see that $v(x, t)$ obeys the same differential equation as $u(x, t)$: 

\begin{align*}
\pddx{u} &= \pddx{v} + \pddx{u_s} = \pddx{v} \\
\pdt{u} &=  \pdt{v} + \pdt{u_s} = \pdt{v}\\
\end{align*}

So we now have
\begin{equation}\label{eq:excess}
\pdt{v} = D\pddx{v}
\end{equation}

Inserting the known boundary and initial conditions: 
\begin{align*}
v(x, 0) &= u(x, 0) - u_s(x) = x - 1 \\
v(0, t) &= u(0, t) - u_s(0) = 1 - 1 = 0 \\
v(d, t) &= v(1, t) = u(1, t) - u_s(1) = 0 
\end{align*}

The partial differential equation for $v(x, t)$ has dirichlet boundary conditions. We can easily solve this using separation of variables $v(x, t) = F(x) G(t)$:
\[
 F(x) = A\sin(\lambda x ) + B\cos(\lambda x) \qquad G(t) = e^{-\lambda^2 t}
\]
Using the given boundary conditions now gives 
\[
 v(0, t) = B = 0 \qquad v(1,t) = A\sin(\lambda) = 0 \, \rightarrow \, \lambda = n\pi
\]
where $n = 1, 2, 3 \ldots$. One solution can therefore be 
\[
 v(x, t) = D_n \sin(n \pi x)e^{-n^2 \pi^2 t}
\]

There are infinitely many of these solutions, and the complete solution is a superposition of these: 
\[
 v(x, t) = \displaystyle \sum_{n=1}^{\infty} D_n \sin(n\pi x) e^{-n^2 \pi^2 t}
\]

We can now use the initial conditions to determine $D_n$: 
\[
 v(x, 0) = \displaystyle \sum_{n=1}^{\infty} D_n \sin(n\pi x) = x - 1
\]
which is a fourier series  with $D_n$ as it's fourier coefficients. So

\begin{align*}
D_n &= \frac{2}{d} \displaystyle \int_0^d (x - 1)\sin(n\pi x) dx \\
&= 2 \displaystyle \int_0^1 x\sin(n\pi x) dx - 2\displaystyle \int_0^1 \sin(n\pi x) dx \\
&= 2\left[ \frac{1}{n^2 \pi^2} \sin(n \pi x) - \frac{x}{n \pi} \cos(n \pi x)\right]_0^1 + 2\left[\frac{1}{n \pi} \cos(n \pi x) \right]_0^1 \\
&= \frac{2}{n^2 \pi^2} \sin(n \pi) - \frac{2}{n\pi} \cos(n\pi) + \frac{2}{n\pi}\cos(n\pi) - \frac{2}{n\pi} \\
&= \frac{2}{n^2 \pi^2}\left[ \sin(n\pi) - n \pi \right]
\end{align*}

Since $\sin(n\pi) = 0$ for $n = 1, 2, 3 \ldots$, the closed form solution of $v(x, t)$ becomes
\begin{equation}
v(x, t) = \displaystyle \sum_{n=1}^\infty \frac{-2}{n\pi} \sin(n\pi x)\,e^{-n^2\pi^2 t}
\end{equation}


\section*{3. Algorithm}


\subsection*{3.1 Explicit Scheme}

The explicit method is the most basic of the three methods compared in this assignment. In essence it is a the forward euler method for each time step. As alway we start with the taylor expansion of $u$ around $ x + \Delta x$ and $t + \Delta t$, giving two expressions for the time derivative and the position derivative: 

 \[
u_t\approx \frac{u(x,t+\Delta t)-u(x,t)}{\Delta t}
\]
and
\[
u_{xx}\approx \frac{u(x+\Delta x,t)-2u(x,t)+u(x-\Delta x,t)}{\Delta x^2},
\]

Defining $u(x, t) = u_{i,j}$ and $u(x + \Delta x, t + \Delta t) = u_{i+1, j+1}$ we can discretize the diffusion equation with the ''standard approximation'':
\begin{equation}\label{eq:explicit}
\frac{u_{i, j+1} - u_{i, j}}{\Delta t} = D\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1, j}}{\Delta x^2} 
\end{equation}
This expression has a local approximation error of $\mathcal{O}(\Delta t)$ and $\mathcal{O}(\Delta x^2)$ from the truncation errors in the taylor expansions. 

This is a very straightforward method to implement. Since we are given boundary conditions and initial conditions that cover all positions at $t = 0$ and all times at $x = 0$ and $x = d$ we know $u_{i, j}$, $u_{i+1, j}$ and $u_{i-1, j}$ for $j=0$ using this we can solve equation (\ref{eq:explicit}) for  $u_{i, j+1}$: 
\[
u_{i, j+1} = \alpha u_{i-1, j} + (1-2\alpha)u_{i, j} + \alpha u_{i+1,j}
\]
where we have defined $\alpha = D \Delta t/\Delta x^2$. This can be rewritten as a matrix-vector multiplication: \[ \vec{V}_j = A \, \vec{V}_{j-1} \]
where \( A = I - \alpha B \), $I$ is the identity matrix, \[ B = \begin{pmatrix}
2 & -1 & 0 & \dots \\
-1 & 2 & -1 & \dots \\
\dots & \dots &  \dots & \dots \\
\dots & 0 & -1 & 2\\
\end{pmatrix} \qquad \mathrm{and} \qquad \vec{V}_j = \begin{pmatrix}
u_{0, j} \\ u_{1, j} \\ \vdots \\u_{n, j} \end{pmatrix}
\]

\subsection*{3.2 Implicit Scheme}

The implicit scheme is based on the backwards euler: 
\[
u_t\approx \frac{u(x,t)-u(x,t-\Delta t)}{\Delta t}
\]
\[
u_{xx}\approx \frac{u(x_i+\Delta x,t_j)-2u(x_i,t_j)+u(x_i-\Delta x,t_j)}{\Delta x^2},
\]
Which results in truncation errors of $\mathcal{O}(\Delta x^2)$ and $\mathcal{O}(\Delta t)$, same as the explicit method. 
The diffusion equation therefore becomes 
\begin{equation}\label{eq:Implicit}
\frac{u_{i, j} - u_{i, j-1} }{\Delta t} = D\frac{u_{i+1,j} - 2u_{i,j} + u_{i-1, j}}{\Delta x^2} 
\end{equation} 
where we can solve for $u_{i, j-1}$: 
\[
u_{i, j-1} = -\alpha u_{i-1, j} + (1+2\alpha)u_{i, j} - \alpha u_{i+1,j}
\]
which in matrix-vector form becomes: 
\[
A\, \vec{V}_j = \vec{V}_{j-1}
\] where \( A = I + \alpha B\) and $B$ is then same $B$ as in the explicit scheme

This is a tridiagonal set of equations where we can solve for $\vec{V}_j$ the same way as in project 1. 

\subsubsection*{3. 2. 1. Solution of a Tridiagonal Matrix Equation (Project 1)}


Given a tridiagonal 
matrix we can use a form of gaussian elimination in order to solve 
for the elements in $\vec{v}$. Performing the matrix operations as
above will give us a set of equations on the form:

$$
a_{i}u_{i-1} + b_{i}u_{i} + c_{i}u_{i+1} = f_{i}
$$

assuming that $a_1 = 0$ and $c_{n} = 0$ we get three ``equations``:


\begin{align*}
b_{1}u_{1} + c_{1}u_{1} = f_{1} \qquad &; i = 1\\
a_{i}u_{i-1} + b_{i}u_{i} + c_{i}u_{i+1} = f_{i} \qquad &; i = 2, \ldots, n-1 \\
a_{n}u_{n} + b_{n}u_{n} = f_{n} \qquad &; i = n
\end{align*}

Using gaussian elimination we are supposed to eliminate all the $u_{i}$ from the equations
for index $j$ with $j < i$. We see from the general equation above that for the first 
three steps this means: 

\begin{align*}
f_1 - \frac{a_1}{b_0}f_0 &= a_1 u_0 + b_1 u_1 + c_1 u_2 - \left(a_1u_0 + \frac{a_1 c_0}{b_0} u_1 \right) \\
&= \left( b_1 - \frac{a_1 c_0}{b_0} \right) u_1 + c_1 u_2 \\ 
&= b'_1 u_1  + c_1 u_2 = f'_1 \\ 
f_2 - \frac{a_2}{b'_1}f'_1 &= a_2 u_1 + b_2 u_2 + c_2 u_3 - \left(a_2 u_1 + \frac{a_2 c_1}{b'_1} u_2 \right) \\ 
&= \left( b_1 - \frac{a_2 c_1}{b'_1} \right) u_2 + c_2 u_3 \\
&= b'_2 u_2 + c_2 u_3  = f'_2 \\
f_3 - \frac{a_3}{b'_2}f'_2 &= a_3 u_2 + b_3 u_3 + c_3 u_4 - \left( a_3 u_2 + \frac{a_3 c_2}{b'_2} u_3\right) \\
&= \left( b_3 - \frac{a_3 c_2}{b'_2} \right) u_3 + c_3 u_4 \\
&= b'_3 u_3 + c_3 u_4 = f'_3
\end{align*}

We see from the above equations that we can obtain recursive formula for the coefficients $b'_i$ and 
$f'_i$: 

\begin{align}
f'_i = b'_i u_i + c-i u_{i+1} = f_i - \frac{a_i}{b'_{i-1}}f'_{i-1} \\
b'_i = \left( b_i - \frac{a_i c_{i-1}}{b'_{i-1}} \right)
\end{align}

which gives us

\begin{align*}
u_{i} = \frac{f'_i - c_i u_{i+1}}{b'_i}
\end{align*}

The two coefficients above can easily be found by a for-loop since $a_1 = 0$ they only depend on each other at 
previous indices and.
Given $c_n = 0$ we see that we have an equation for $u_n$ which does not depend on $u_{n+1}$, and 
from these equations we can find $u_{i}$ as a function of $u_{i+1}$ which we can backwards substitute 
from $i=n$ to $i=1$.

\subsection*{3.3 Crank-Nicolson}

It is possible to combine the explicit and implicit. Introducing the parameter $\theta$ we can rewrite these schemes into a more general approach
\[
\frac{\theta}{\Delta x^2}(u_{i+1,j} - 2u_{i,j} + u_{i-1, j}) + \frac{1 - \theta}{\Delta x^2}(u_{i+1,j-1} - 2u_{i,j-1} + u_{i-1, j-1}) = \frac{1}{\Delta t}(u_{i,j} - u_{i, j-1})
\]
with $\theta = 0$ this reduces to the explicit scheme, $\theta = 1$ yields the implicit method and for $\theta = 1/2$ we get the Crank-Nicolson scheme, which has a truncation errors of $\mathcal{O}(\Delta x^2)$ and $\mathcal{O}(\Delta t^2)$, same as the explicit method. Using the same notation as above this yields
\begin{equation} \label{eq:CN}
-\alpha u_{i-1, j} + (2 + 2\alpha)u_{i,j} - \alpha u_{i+1, j} = \alpha u_{i-1, j-1} + (2 - 2\alpha)u_{i, j-1} + \alpha u_{i+1, j-1}
\end{equation}

Which we see can be rewritten as the matrix-vector equation 
\begin{equation}
(2I + \alpha B) \vec{V}_j = (2I - \alpha B) \vec{V}_{j-1}
\end{equation} where $B$ and $I$ are the same matrices as in the schemes above.

Defining a vector $\vec{W}$ as \( \vec{W} = (2I - \alpha B) \vec{V}_{j-1} \) we see that we end up with a tridiagonal equation system of the same form as in the implicit method, where we can now solve the matrix-vector equation \( (2I + \alpha B) \vec{V}_j = \vec{W}\) for $\vec{V}_j$

\subsection*{3.4 Stability}

All the methods above are stable if the spectral radius of the matrix $A$, $\rho(A)$, is less than 1. The spectral radius is given by $\rho(A) = \mathrm{max}\{ |\lambda | : \mathrm{det}(A - I\lambda) = 0 \}$, and can be interpreted as the the smallest radius of a sphere in the complex plane, centred on the origin, which contains all of the matrix $A$'s eigenvalues. If the matrix $A$ is positive definite, e.g. has only positive eigenvalues, this condition is always satisfied. 

\subsubsection*{3.4.1 Explicit method}

For the explicit method we have that $A = I - \alpha B$, where \(B\) is the same as in previous sections. The eigenvalues of this matrix therefore are \( \lambda_i = 1 - \alpha \mu_i \), where \(\mu_i\) are the eigenvalues of the matrix \(B\). To find \(\mu_i\) we can express the matrix elements of \(B\) as \( b_ {i,j} = 2\delta_{i, j} - \delta_{i+1, j} - \delta_{i-1, j} \). 

We now have a set of eigenequations for the components i of \(B\): 
\begin{align*}
(B\vec{x})_i &= \mu_i x_i \\
&= \displaystyle \sum_{j=1}^n (2\delta_{i, j} - \delta_{i+1, j} - \delta_{i-1, j})x_j \\
&= 2x_i - x_{i+1} - x_{i-1}
\end{align*}

Assuming that $\vec{x}$ can be expanded in a basis of \( \vec{x} = (\sin(\theta), \sin(2\theta) \ldots \sin(n\theta)) \), where $\theta = l\pi/ (n+1)$. The elements in \(B\) can now be expressed as 
\begin{align*}
2\sin(i\theta) -\sin([i+1]\theta) - \sin([i-1]\theta) &= \mu_i \sin(i\theta)\\
2(1 - \cos\theta)\sin(i\theta) &= \mu_i \sin(i\theta)\\
\end{align*}\begin{equation} \label{eq:mu}
\mu_i = 2(1 - \cos\theta)
\end{equation}

The eigenvalues of \(A\) can be expressed as \( \lambda_i = 1 - \alpha \mu_i = 1 - 2\alpha(1 - \cos \theta)\). We see that \(A\) is not a positive definite matrix, and the spectral radius condition can be expressed as 
\[
\rho(A) < 1 \longrightarrow |\lambda_i| < 1 
\]\begin{align*}
-1 &< 1 - \alpha \mu_i < 1 \\
-1 &< 1 - 2\alpha(1 - \cos \theta) < 1 \\
\end{align*}
which is satisfied if \( \alpha \, < \, [1 - \cos(\theta)]^{-1} \longrightarrow \alpha \, \le \,  1/2  \), and \( \Delta t /\Delta x^2 \, \le \,  1/2 \)


\subsubsection*{3.4.2 Implicit method}

The matrix equation in the implicit method method is given by \( A = I + \alpha B\), so the eigenvalues become \( \lambda_i = 1 + \alpha \mu_i \), where \( \mu_i \) is given by the same expression as in equation (\ref{eq:mu}), giving \( \lambda_i = 1 + 2\alpha (1 - \cos \theta) \). Since \( 1 - \cos \theta >  0\) for all \( \theta \). Our spectral radius is given by \(\rho(A^{-1}) \), giving \( |[1 + 2\alpha (1 - \cos \theta)]^{-1}| < 1\), which is true for all \(\theta\), since \( 0 \le 1 - \cos \theta \le 1\).

\subsubsection*{3.4.3 Crank-Nicolson}

For the Crank-Nicolson scheme we have \( (2I + \alpha B) \vec{V}_j = (2I - \alpha B) \vec{V}_{j-1}\), meaning that the matrix \(A = (2I + \alpha B)^{-1}(2I - \alpha B)\). The condition \(\rho(A) < 1 \) becomes \( \rho((2I + \alpha B)^{-1}(2I - \alpha B)) < 1 \, \, \rightarrow\, \,   |(2 + \alpha \mu_i)^{-1}(2 - \alpha \mu_i)|\, \,  < 1\):
\begin{align*}
\left| \frac{2 - \alpha \mu_i }{\alpha \mu_i +2 }\right| &< 1 \\
| \alpha \mu_i &< \alpha \mu_i + 2 \\
| 2\alpha (1 - \cos \theta) - 2 | &< 2\alpha (1 - \cos \theta) + 2
\end{align*}
since \( 0 \le 1 - \cos \theta \le 1\) is true for all \(\theta\), and \( \alpha > 0\), this will always be true. 

\section*{4. Results}

The results of the three methods are plotted together with the analytical solution in figure (\ref{fig:solution}). It appears from like the explicit solution is slightly less accurate than the other two solutions, while the Crank-Nicolson scheme does not produce any noticeably better results than the implicit method.

These results seem to indicate that the concentration of neurotransmitters across the synaptic cleft separating the cell membranes of two cells seem to converge to the steady state solution \( u_s = 1 - x \).

The results from these methods are very inaccurate for the time and position steps given in the assignment note, but increasing \( \Delta x \) from 1/10 to 1/50 makes the lines overlap almost completely at \(t = 0.05\), where the inaccuracy is clearest for bigger position steps (the time step \( \Delta t\) is calculated from the stability criterion for the explicit method).
At t = 0.8 there are no noticeable difference when decreasing the time step, and as expected they overlap completely, identical to the steady state solution \( u_s \).

There still seems to be somewhat of a gap between the solutions, where the numerical methods overlap, while the analytical solution ends up slightly lower than the numerical ones. This gap decreases when we decrease the position step, so it is most likely a result of the inaccuracy of the methods. 

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{Solution1.png}
\includegraphics[scale=0.5]{Solution2.png}
\caption{The explicit, implicit and Crank-Nicolson scheme plotted together with the analytical solution of the diffusion equation, for \(t \approx  0.05 \, \, \mathrm{and} \, \, t \approx 0.9\)}
\label{fig:solution}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{Solution1_zoom.png}
\caption{Solution at $ t = 0.05 $ with the ''tail-end'' enlarged.}
\label{fig:solution zoom}
\end{figure}


\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{Solution1_zoom2.png}
\caption{Solution at $ t = 0.05 $ magnified around x = 0.21.}
\label{fig:solution zoom1}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{Solution1_zoom3.png}
\caption{Solution at $ t = 0.05 $  magnified around x = 7.26.}
\label{fig:solution zoom2}
\end{figure}


\section*{5. Conclusion \& Comments}

There does not seem to be any real difference between the methods for time steps small enough to produce accurate results. All three methods produce more or less the same result. Enlarging the tail end of the \(t = 0.05\) plot even more it would appear as though the explicit method is more accurate than the two other (figure (\ref{fig:solution zoom1})). Moving further down the x-axis on the other hand, the distance from the analytical to the numerical solution seem to vary, and far enough up the Crank-Nicolson and implicit method seem to alternately produce the most accurate results (figure (\ref{fig:solution zoom2})). It would therefore appear that these methods, at least with the problem we are currently studying, are equally accurate. Assuming, of course, the methods are implemented correctly, which do not necessarily need to be the case, but the results seem to be ''right enough'' to indicate correct implementation.


\end{document}